{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "231a78cd-0c9f-4108-ae2c-3c88fa757488",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = 'Natural Language Processing (NLP) allows machines to understand human language. Tokenization is the first step in many NLP tasks. It splits text into words, sentences, or other meaningful units.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e25975ad-6d86-4ece-a407-61777b19aaa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Natural Language Processing (NLP) allows machines to understand human language. Tokenization is the first step in many NLP tasks. It splits text into words, sentences, or other meaningful units.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca41e04b-1d69-468c-ba4c-19994e2f31df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " '(NLP)',\n",
       " 'allows',\n",
       " 'machines',\n",
       " 'to',\n",
       " 'understand',\n",
       " 'human',\n",
       " 'language.',\n",
       " 'Tokenization',\n",
       " 'is',\n",
       " 'the',\n",
       " 'first',\n",
       " 'step',\n",
       " 'in',\n",
       " 'many',\n",
       " 'NLP',\n",
       " 'tasks.',\n",
       " 'It',\n",
       " 'splits',\n",
       " 'text',\n",
       " 'into',\n",
       " 'words,',\n",
       " 'sentences,',\n",
       " 'or',\n",
       " 'other',\n",
       " 'meaningful',\n",
       " 'units.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8963a26-1606-4f34-8e22-f031f9da258e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural Language Processing (NLP) allows machines to understand human language',\n",
       " ' Tokenization is the first step in many NLP tasks',\n",
       " ' It splits text into words, sentences, or other meaningful units',\n",
       " '']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98922b7d-c770-4938-83d6-894b3e2cb570",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Hello!, How are you?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "700da3d1-8833-4033-b4aa-70c7447769ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello!,', 'How', 'are', 'you?']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34db88fa-5a70-4a7e-82db-eb1e2fb86838",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8323140-9f7d-42e6-a246-a4ca9b58e9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b07ac60-5e7b-4484-aaa2-9fc4104439ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " '(',\n",
       " 'NLP',\n",
       " ')',\n",
       " 'allows',\n",
       " 'machines',\n",
       " 'to',\n",
       " 'understand',\n",
       " 'human',\n",
       " 'language',\n",
       " '.',\n",
       " 'Tokenization',\n",
       " 'is',\n",
       " 'the',\n",
       " 'first',\n",
       " 'step',\n",
       " 'in',\n",
       " 'many',\n",
       " 'NLP',\n",
       " 'tasks',\n",
       " '.',\n",
       " 'It',\n",
       " 'splits',\n",
       " 'text',\n",
       " 'into',\n",
       " 'words',\n",
       " ',',\n",
       " 'sentences',\n",
       " ',',\n",
       " 'or',\n",
       " 'other',\n",
       " 'meaningful',\n",
       " 'units',\n",
       " '.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24632bba-dc13-497d-87a4-bc2d3cdf001d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural Language Processing (NLP) allows machines to understand human language.',\n",
       " 'Tokenization is the first step in many NLP tasks.',\n",
       " 'It splits text into words, sentences, or other meaningful units.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "797df712-03e1-46bc-b7db-155f1eab1bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.8.7-cp39-cp39-win_amd64.whl.metadata (28 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.13-cp39-cp39-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.11-cp39-cp39-win_amd64.whl.metadata (8.8 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.10-cp39-cp39-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.6-cp39-cp39-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.1-cp39-cp39-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.16.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\hp\\anaconda3\\envs\\gpu_support\\lib\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\hp\\anaconda3\\envs\\gpu_support\\lib\\site-packages (from spacy) (1.23.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\hp\\anaconda3\\envs\\gpu_support\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Downloading pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hp\\anaconda3\\envs\\gpu_support\\lib\\site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hp\\anaconda3\\envs\\gpu_support\\lib\\site-packages (from spacy) (78.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\anaconda3\\envs\\gpu_support\\lib\\site-packages (from spacy) (24.2)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading pydantic_core-2.33.2-cp39-cp39-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\hp\\anaconda3\\envs\\gpu_support\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\anaconda3\\envs\\gpu_support\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\envs\\gpu_support\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\anaconda3\\envs\\gpu_support\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\envs\\gpu_support\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.4.26)\n",
      "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.3.0.tar.gz (2.5 MB)\n",
      "     ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "     -------- ------------------------------- 0.5/2.5 MB 8.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.5/2.5 MB 11.1 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting numpy>=1.19.0 (from spacy)\n",
      "  Downloading numpy-2.0.2-cp39-cp39-win_amd64.whl.metadata (59 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\envs\\gpu_support\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\hp\\anaconda3\\envs\\gpu_support\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.21.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: wrapt in c:\\users\\hp\\anaconda3\\envs\\gpu_support\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.0)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.2.1-cp39-cp39-win_amd64.whl.metadata (9.3 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\hp\\anaconda3\\envs\\gpu_support\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hp\\anaconda3\\envs\\gpu_support\\lib\\site-packages (from jinja2->spacy) (3.0.2)\n",
      "Downloading spacy-3.8.7-cp39-cp39-win_amd64.whl (14.9 MB)\n",
      "   ---------------------------------------- 0.0/14.9 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 3.4/14.9 MB 15.4 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 7.9/14.9 MB 18.0 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 11.8/14.9 MB 18.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.9/14.9 MB 18.7 MB/s eta 0:00:00\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.11-cp39-cp39-win_amd64.whl (39 kB)\n",
      "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Downloading murmurhash-1.0.13-cp39-cp39-win_amd64.whl (24 kB)\n",
      "Downloading preshed-3.0.10-cp39-cp39-win_amd64.whl (118 kB)\n",
      "Downloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Downloading pydantic_core-2.33.2-cp39-cp39-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.0/2.0 MB 27.3 MB/s eta 0:00:00\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.1-cp39-cp39-win_amd64.whl (633 kB)\n",
      "   ---------------------------------------- 0.0/633.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 633.4/633.4 kB 24.7 MB/s eta 0:00:00\n",
      "Downloading thinc-8.3.6-cp39-cp39-win_amd64.whl (1.8 MB)\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.8/1.8 MB 12.1 MB/s eta 0:00:00\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading numpy-2.0.2-cp39-cp39-win_amd64.whl (15.9 MB)\n",
      "   ---------------------------------------- 0.0/15.9 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 2.9/15.9 MB 15.2 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 7.3/15.9 MB 17.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 13.1/15.9 MB 20.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 14.2/15.9 MB 18.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.9/15.9 MB 17.6 MB/s eta 0:00:00\n",
      "Downloading typer-0.16.0-py3-none-any.whl (46 kB)\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Downloading cloudpathlib-0.21.1-py3-none-any.whl (52 kB)\n",
      "Downloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "   ------------------------- -------------- 3.4/5.4 MB 18.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 5.0/5.4 MB 13.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.4/5.4 MB 12.1 MB/s eta 0:00:00\n",
      "Downloading marisa_trie-1.2.1-cp39-cp39-win_amd64.whl (152 kB)\n",
      "Downloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Building wheels for collected packages: blis\n",
      "  Building wheel for blis (pyproject.toml): started\n",
      "  Building wheel for blis (pyproject.toml): finished with status 'error'\n",
      "Failed to build blis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Building wheel for blis (pyproject.toml) did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [32 lines of output]\n",
      "  BLIS_COMPILER? None\n",
      "  C:\\Users\\HP\\AppData\\Local\\Temp\\pip-build-env-mv2y1x6u\\overlay\\Lib\\site-packages\\setuptools\\dist.py:759: SetuptoolsDeprecationWarning: License classifiers are deprecated.\n",
      "  !!\n",
      "  \n",
      "          ********************************************************************************\n",
      "          Please consider removing the following classifiers in favor of a SPDX license expression:\n",
      "  \n",
      "          License :: OSI Approved :: BSD License\n",
      "  \n",
      "          See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.\n",
      "          ********************************************************************************\n",
      "  \n",
      "  !!\n",
      "    self._finalize_license_expression()\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\\lib.win-amd64-cpython-39\\blis\n",
      "  copying blis\\about.py -> build\\lib.win-amd64-cpython-39\\blis\n",
      "  copying blis\\benchmark.py -> build\\lib.win-amd64-cpython-39\\blis\n",
      "  copying blis\\__init__.py -> build\\lib.win-amd64-cpython-39\\blis\n",
      "  creating build\\lib.win-amd64-cpython-39\\blis\\tests\n",
      "  copying blis\\tests\\common.py -> build\\lib.win-amd64-cpython-39\\blis\\tests\n",
      "  copying blis\\tests\\test_dotv.py -> build\\lib.win-amd64-cpython-39\\blis\\tests\n",
      "  copying blis\\tests\\test_gemm.py -> build\\lib.win-amd64-cpython-39\\blis\\tests\n",
      "  copying blis\\tests\\__init__.py -> build\\lib.win-amd64-cpython-39\\blis\\tests\n",
      "  copying blis\\cy.pyx -> build\\lib.win-amd64-cpython-39\\blis\n",
      "  copying blis\\py.pyx -> build\\lib.win-amd64-cpython-39\\blis\n",
      "  copying blis\\cy.pxd -> build\\lib.win-amd64-cpython-39\\blis\n",
      "  copying blis\\__init__.pxd -> build\\lib.win-amd64-cpython-39\\blis\n",
      "  running build_ext\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for blis\n",
      "ERROR: Failed to build installable wheels for some pyproject.toml based projects (blis)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5db3974-8a90-41ce-ba92-59ffd223e0eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m doc1 \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m(text1)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nlp' is not defined"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd3f9a7-81fd-4dc6-92c2-5c4d4b83dc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9809db49-fdf8-41fe-8346-7d2037475d01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
