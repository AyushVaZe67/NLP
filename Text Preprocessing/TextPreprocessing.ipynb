{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c35b049d-cd1b-400f-9487-7e760b9afe7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The QUICK Brown FoX Jumps Over The LAZY Dog!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd933e86-7693-43db-91c8-164266d0d2d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the quick brown fox jumps over the lazy dog!'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a17da7cb-e16b-4e46-a3a2-0b88d4968479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "welcome to my websitethis is a sample paragraph!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "html_text = \"<h1>Welcome TO <b>My Website</b></h1><p>This is a SAMPLE Paragraph!</p>\"\n",
    "\n",
    "# Remove HTML tags using regex\n",
    "clean_text = re.sub(r'<[^>]+>', '', html_text)\n",
    "\n",
    "# Lowercase the result\n",
    "clean_text = clean_text.lower()\n",
    "\n",
    "print(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dba5a967-5a13-4e46-b5b9-f3f8e230044a",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = '<a href=\"https://example.com\">Click here</a> to visit our <a href=\"https://blog.example.com\">blog</a>.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a174628a-c31b-40d8-af70-74afa5b9126a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to visit our .\n"
     ]
    }
   ],
   "source": [
    "clean_text = re.sub(r'<a [^>]*>.*?</a>', '', html)\n",
    "print(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc93cafa-6f53-4d94-87e0-d4b19246d969",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello there! How's everything going? I'm working on GenAI, NLP, etc. It's fun, right?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7ad6781-069b-4523-ba6e-1ddb8f5c56b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello there Hows everything going Im working on GenAI NLP etc Its fun right\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "clean_text = ''.join(char for char in text if char not in string.punctuation)\n",
    "\n",
    "print(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46778371-d818-4e23-9673-2c8b97ff602e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: GN! Please reply ASAP. FYI, the docs are ready. TTYL!\n",
      "Cleaned Words: ['gn', 'please', 'reply', 'asap', 'fyi', 'the', 'docs', 'are', 'ready', 'ttyl']\n",
      "Expanded Words: ['good night', 'please', 'reply', 'as soon as possible', 'for your information', 'the', 'docs', 'are', 'ready', 'talk to you later']\n",
      "Expanded Text: good night please reply as soon as possible for your information the docs are ready talk to you later\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# Sample text with abbreviations\n",
    "text = \"GN! Please reply ASAP. FYI, the docs are ready. TTYL!\"\n",
    "\n",
    "# Dictionary for common abbreviations\n",
    "abbreviations = {\n",
    "    'gn': 'good night',\n",
    "    'asap': 'as soon as possible',\n",
    "    'fyi': 'for your information',\n",
    "    'ttyl': 'talk to you later'\n",
    "}\n",
    "\n",
    "# 1. Lowercase & remove punctuation\n",
    "clean_text = ''.join(char for char in text.lower() if char not in string.punctuation)\n",
    "\n",
    "# 2. Split into words\n",
    "words = clean_text.split()\n",
    "\n",
    "# 3. Replace abbreviations\n",
    "expanded_words = [abbreviations.get(word, word) for word in words]\n",
    "\n",
    "# 4. Rejoin expanded words\n",
    "expanded_text = ' '.join(expanded_words)\n",
    "\n",
    "# Output\n",
    "print(\"Original Text:\", text)\n",
    "print(\"Cleaned Words:\", words)\n",
    "print(\"Expanded Words:\", expanded_words)\n",
    "print(\"Expanded Text:\", expanded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77e96402-ec2c-4e2a-91ca-b67d6c9cae1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: I relly love wrting code in Pythn. Itz awsome!\n",
      "Corrected Text: I really love writing code in Myth. Tz some!\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# Sample text with spelling errors\n",
    "text = \"I relly love wrting code in Pythn. Itz awsome!\"\n",
    "\n",
    "# Create a TextBlob object\n",
    "blob = TextBlob(text)\n",
    "\n",
    "# Apply spell correction\n",
    "corrected_text = blob.correct()\n",
    "\n",
    "# Output\n",
    "print(\"Original Text:\", text)\n",
    "print(\"Corrected Text:\", corrected_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b501fdae-4c26-47ae-aaf9-fc7dbe899ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Words: ['this', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n",
      "Filtered Words: ['sample', 'sentence', 'showing', 'stop', 'words', 'filtration']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download stopwords and punkt tokenizer (only once)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Sample text\n",
    "text = \"This is a sample sentence, showing off the stop words filtration.\"\n",
    "\n",
    "# Tokenize the text\n",
    "words = word_tokenize(text.lower())  # convert to lowercase and tokenize\n",
    "\n",
    "# Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [word for word in words if word.isalnum() and word not in stop_words]\n",
    "\n",
    "# Output\n",
    "print(\"Original Words:\", words)\n",
    "print(\"Filtered Words:\", filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85f65dd0-58ab-4049-a49e-09f802567fed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['au',\n",
       " 'aux',\n",
       " 'avec',\n",
       " 'ce',\n",
       " 'ces',\n",
       " 'dans',\n",
       " 'de',\n",
       " 'des',\n",
       " 'du',\n",
       " 'elle',\n",
       " 'en',\n",
       " 'et',\n",
       " 'eux',\n",
       " 'il',\n",
       " 'ils',\n",
       " 'je',\n",
       " 'la',\n",
       " 'le',\n",
       " 'les',\n",
       " 'leur',\n",
       " 'lui',\n",
       " 'ma',\n",
       " 'mais',\n",
       " 'me',\n",
       " 'même',\n",
       " 'mes',\n",
       " 'moi',\n",
       " 'mon',\n",
       " 'ne',\n",
       " 'nos',\n",
       " 'notre',\n",
       " 'nous',\n",
       " 'on',\n",
       " 'ou',\n",
       " 'par',\n",
       " 'pas',\n",
       " 'pour',\n",
       " 'qu',\n",
       " 'que',\n",
       " 'qui',\n",
       " 'sa',\n",
       " 'se',\n",
       " 'ses',\n",
       " 'son',\n",
       " 'sur',\n",
       " 'ta',\n",
       " 'te',\n",
       " 'tes',\n",
       " 'toi',\n",
       " 'ton',\n",
       " 'tu',\n",
       " 'un',\n",
       " 'une',\n",
       " 'vos',\n",
       " 'votre',\n",
       " 'vous',\n",
       " 'c',\n",
       " 'd',\n",
       " 'j',\n",
       " 'l',\n",
       " 'à',\n",
       " 'm',\n",
       " 'n',\n",
       " 's',\n",
       " 't',\n",
       " 'y',\n",
       " 'été',\n",
       " 'étée',\n",
       " 'étées',\n",
       " 'étés',\n",
       " 'étant',\n",
       " 'étante',\n",
       " 'étants',\n",
       " 'étantes',\n",
       " 'suis',\n",
       " 'es',\n",
       " 'est',\n",
       " 'sommes',\n",
       " 'êtes',\n",
       " 'sont',\n",
       " 'serai',\n",
       " 'seras',\n",
       " 'sera',\n",
       " 'serons',\n",
       " 'serez',\n",
       " 'seront',\n",
       " 'serais',\n",
       " 'serait',\n",
       " 'serions',\n",
       " 'seriez',\n",
       " 'seraient',\n",
       " 'étais',\n",
       " 'était',\n",
       " 'étions',\n",
       " 'étiez',\n",
       " 'étaient',\n",
       " 'fus',\n",
       " 'fut',\n",
       " 'fûmes',\n",
       " 'fûtes',\n",
       " 'furent',\n",
       " 'sois',\n",
       " 'soit',\n",
       " 'soyons',\n",
       " 'soyez',\n",
       " 'soient',\n",
       " 'fusse',\n",
       " 'fusses',\n",
       " 'fût',\n",
       " 'fussions',\n",
       " 'fussiez',\n",
       " 'fussent',\n",
       " 'ayant',\n",
       " 'ayante',\n",
       " 'ayantes',\n",
       " 'ayants',\n",
       " 'eu',\n",
       " 'eue',\n",
       " 'eues',\n",
       " 'eus',\n",
       " 'ai',\n",
       " 'as',\n",
       " 'avons',\n",
       " 'avez',\n",
       " 'ont',\n",
       " 'aurai',\n",
       " 'auras',\n",
       " 'aura',\n",
       " 'aurons',\n",
       " 'aurez',\n",
       " 'auront',\n",
       " 'aurais',\n",
       " 'aurait',\n",
       " 'aurions',\n",
       " 'auriez',\n",
       " 'auraient',\n",
       " 'avais',\n",
       " 'avait',\n",
       " 'avions',\n",
       " 'aviez',\n",
       " 'avaient',\n",
       " 'eut',\n",
       " 'eûmes',\n",
       " 'eûtes',\n",
       " 'eurent',\n",
       " 'aie',\n",
       " 'aies',\n",
       " 'ait',\n",
       " 'ayons',\n",
       " 'ayez',\n",
       " 'aient',\n",
       " 'eusse',\n",
       " 'eusses',\n",
       " 'eût',\n",
       " 'eussions',\n",
       " 'eussiez',\n",
       " 'eussent']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('french')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553ec749-89e4-45a0-a7a2-27ff01c43beb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
